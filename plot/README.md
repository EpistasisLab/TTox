# This folder contains figures generated by the repository.

## Visualize performance of binding prediction models incorporated with feature selection 

+ First, comparison of training performance was conducted between hyperparameter settings, in order to the demonstrate the individual effect of each hyperparameter. The analysis was conducted separately on two sets: i) datasets with molecular descriptors as features and ii) datasets with MACCS fingerprints as features. The comparison was then visualized in the form of boxplots, and stored in [`compound_target_0.25_binary_feature_select_tuning/`](compound_target_0.25_binary_feature_select_tuning/). Plotting files of the first category start with 'descriptor_all'. Plotting files of the second category start with 'fingerprint_maccs'. Each boxplot file demonstrate the comparison of a single hyperparameter. In each boxplot, the performance distributions shown in two adjacent boxes only differ in the compared hyperparameter. The following four types of hyperparameters were compared:
  + the method used to rank features (MultiSURF vs MultiSURFstar, files that end with 'ranking_method_compare.pdf')
  + whether to implement iterative scoring (TURF) when ranking features (TURF vs no TURF, files that end with 'implement_TURF_compare.pdf')
  + the method used to build classification models (RandomForest vs XGBoost, file that end with 'learning_method_compare.pdf')
  + the value of tolerance score used to determine which subset of features maximizes model performance (20 vs 50, file that end with 'tolerance_compare.pdf')
  
+ Next, the performance of our feature selection pipeline was compared against other generic models. The analysis was conducted separately on two sets: i) datasets with molecular descriptors as features (plotting files in [`compound_target_0.25_binary_feature_select_implementation/descriptor_all_analysis/`](compound_target_0.25_binary_feature_select_implementation/descriptor_all_analysis/)) and ii) datasets with MACCS fingerprints as features (plotting files in [`compound_target_0.25_binary_feature_select_implementation/fingerprint_maccs_analysis/`](compound_target_0.25_binary_feature_select_implementation/fingerprint_maccs_analysis/)). Two forms of comparison were made: 
  + comparison of selected feature numbers between ReBATE selection and L1 selection, shown in boxplot (file that ends with 'feature_number_compared.pdf') 
  + comparison of model performance between: i) our pipeline vs generic model without feature selection (files that contain 'all_testing_performance_compared'); ii) our pipeline (ReBATE selection + Randomforest) vs L1 selection + Randomforest (files that contain 'randomforest_l1_testing_performance_compared'); and iii) our pipeline (ReBATE selection + Randomforest) vs L1 selection + logistic regression (files that contain 'lasso_l1_testing_performance_compared'). The comparison was visualized in the form of scatter plot, where the performance of our pipeline is shown in y axis and the performance of the compared item is shown in x axis. Three types of metrics were adopted to evaluate model performance:
    + AUROC (files that end with 'auc.pdf')
    + balanced accuracy (files that end with 'bac.pdf')
    + F1 score (files that end with 'f1.pdf')

+ Last, the performance of our feature selection pipeline was compared across a subset of datasets. The analysis was only conducted for datasets with molecular descriptors as features, as our feature selection pipeline performs better on these datasets in general. Testing AUROC of 0.85 was adopted as threshold to select the subset of feature selection results for this analysis. The visualization plots were stored in [`compound_target_0.25_binary_feature_select_implementation/descriptor_all_compare/`](compound_target_0.25_binary_feature_select_implementation/descriptor_all_compare). The following forms of comparison were made: 
  + comparison of testing AUROC distribution across different measurement types (units of response data), shown in [boxplot](compound_target_0.25_binary_feature_select_implementation/descriptor_all_compare/descriptor_all_select_features_mc_0.85_performance_by_measure.pdf)
  + comparison of testing AUROC distribution across different target function classes, shown in [boxplot](compound_target_0.25_binary_feature_select_implementation/descriptor_all_compare/descriptor_all_select_features_mc_0.85_performance_by_class.pdf). For detailed information about function class annotation, check [this repository](https://github.com/yhao-compbio/target)
  + function class distribution of available targets among all dataets, shown in [pie chart](compound_target_0.25_binary_feature_select_implementation/descriptor_all_compare/descriptor_all_select_features_mc_0.85_target_proportion_by_class.pdf) 
  + comparison of intergroup and intragroup feature similarity among target function classes, shown in [boxplot](compound_target_0.25_binary_feature_select_implementation/descriptor_all_compare/descriptor_all_select_features_mc_0.85_function_similarity_by_class_boxplot.pdf)

## Visualize performance of adverse event prediction models incorporated with feature selection 

+ The performance of our feature selection pipeline was compared against other generic models. Testing AUROC was adopted to evaluate model performance. The comparison was then visualized in the form of scatter plot, where the performance of our pipeline is shown in y axis and the performance of the compared item is shown in x axis. The analysis was conducted for all compound target-adverse event datasets and the visualization plots were stored in [`compound_target_all_adverse_event_feature_select_implementation/`] (compound_target_all_adverse_event_feature_select_implementation/). Two forms of comparison were made:
  + our pipeline vs generic model without feature selection, shown in [this plot](compound_target_all_adverse_event_feature_select_implementation/descriptor_all_all_adverse_event_1_testing_performance_compare_all_select.pdf)
  + our pipeline vs generic model using molecular descriptors as features, shown in [this plot](compound_target_all_adverse_event_feature_select_implementation/descriptor_all_all_adverse_event_1_testing_performance_compare_structure_select.pdf)
